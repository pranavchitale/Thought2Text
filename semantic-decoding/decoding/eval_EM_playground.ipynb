{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import argparse\n",
    "import h5py\n",
    "\n",
    "import config\n",
    "from GPT import GPT\n",
    "from EncodingModel import EncodingModel\n",
    "from StimulusModel import StimulusModel, get_lanczos_mat, affected_trs, LMFeatures\n",
    "from utils_stim import get_stim\n",
    "from utils_resp import get_resp\n",
    "from utils_ridge.ridge import ridge, bootstrap_ridge\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--subject\", type = str, required = True)\n",
    "parser.add_argument(\"--gpt\", type = str, default = \"perceived\")\n",
    "parser.add_argument(\"--experiment\", type = str, default = \"perceived_speech\")\n",
    "parser.add_argument(\"--task\", type = str, default = \"wheretheressmoke\")\n",
    "parser.add_argument(\"--sessions\", nargs = \"+\", type = int, default = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 18, 20])\n",
    "args = parser.parse_args(\"--subject S1\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine word rate model voxels based on experiment\n",
    "word_rate_voxels = \"speech\" if args.experiment in [\"imagined_speech\", \"perceived_movies\"] else \"auditory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models\n",
    "load_location = os.path.join(config.MODEL_DIR, args.subject)\n",
    "word_rate_model = np.load(os.path.join(load_location, \"word_rate_model_%s.npz\" % word_rate_voxels), allow_pickle = True)\n",
    "encoding_model = np.load(os.path.join(load_location, \"encoding_model_%s.npz\" % args.gpt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load responses\n",
    "hf = h5py.File(os.path.join(config.DATA_TEST_DIR, \"test_response\", args.subject, args.experiment, args.task + \".hf5\"), \"r\")\n",
    "resp = np.nan_to_num(hf[\"data\"][:])\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load EM\n",
    "weights = encoding_model[\"weights\"]\n",
    "noise_model = encoding_model[\"noise_model\"]\n",
    "tr_stats = encoding_model[\"tr_stats\"]\n",
    "word_stats = encoding_model[\"word_stats\"]\n",
    "em = EncodingModel(resp, weights, encoding_model[\"voxels\"], noise_model, device = config.EM_DEVICE)\n",
    "em.set_shrinkage(config.NM_ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert args.task not in encoding_model[\"stories\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training stories\n",
    "stories = []\n",
    "with open(os.path.join(config.DATA_TRAIN_DIR, \"sess_to_story.json\"), \"r\") as f:\n",
    "    sess_to_story = json.load(f) \n",
    "for sess in args.sessions:\n",
    "    stories.extend(sess_to_story[str(sess)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rresp = get_resp(args.subject, stories, stack = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27449, 81126)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rresp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Stimulus Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load gpt\n",
    "with open(os.path.join(config.DATA_LM_DIR, args.gpt, \"vocab.json\"), \"r\") as f:\n",
    "    gpt_vocab = json.load(f)\n",
    "gpt = GPT(path = os.path.join(config.DATA_LM_DIR, args.gpt, \"model\"), vocab = gpt_vocab, device = config.GPT_DEVICE)\n",
    "features = LMFeatures(model = gpt, layer = config.GPT_LAYER, context_words = config.GPT_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate encoding model\n",
    "rstim, tr_stats, word_stats = get_stim(stories, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27449, 3072)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rstim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate P(R|S)\n",
    "In other words: \"find the probability of a response given a stimulus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m likelihoods \u001b[38;5;241m=\u001b[39m \u001b[43mem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrstim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrresp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/chronos_data/rrao/Thought2Text/semantic-decoding/decoding/EncodingModel.py:24\u001b[0m, in \u001b[0;36mEncodingModel.prs\u001b[0;34m(self, stim, trs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"compute P(R | S) on affected TRs for each hypothesis\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(): \n\u001b[0;32m---> 24\u001b[0m     stim \u001b[38;5;241m=\u001b[39m \u001b[43mstim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     25\u001b[0m     diff \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(stim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresp[trs] \u001b[38;5;66;03m# encoding model residuals\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     multi \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(torch\u001b[38;5;241m.\u001b[39mmatmul(diff, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision), diff\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'float'"
     ]
    }
   ],
   "source": [
    "likelihoods = em.prs(rstim, rresp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nchunks = int(np.ceil(rresp.shape[0] / 5 / config.CHUNKLEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, alphas, bscorrs = bootstrap_ridge(rstim, rresp, use_corr = False, alphas = config.ALPHAS, nboots = config.NBOOTS, chunklen = config.CHUNKLEN, nchunks = nchunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3072, 81126)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/workspace/weights/S1.npy', weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "bscorrs = bscorrs.mean(2).max(0)\n",
    "vox = np.sort(np.argsort(bscorrs)[-config.VOXELS:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_dict = {story : get_stim([story], features, tr_stats = tr_stats) for story in stories}\n",
    "resp_dict = get_resp(args.subject, stories, stack = False, vox = vox)\n",
    "noise_model = np.zeros([len(vox), len(vox)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 138min 16.9s\n",
    "for hstory in stories:\n",
    "    tstim, hstim = np.vstack([stim_dict[tstory] for tstory in stories if tstory != hstory]), stim_dict[hstory]\n",
    "    tresp, hresp = np.vstack([resp_dict[tstory] for tstory in stories if tstory != hstory]), resp_dict[hstory]\n",
    "    bs_weights = ridge(tstim, tresp, alphas[vox])\n",
    "    resids = hresp - hstim.dot(bs_weights)\n",
    "    bs_noise_model = resids.T.dot(resids)\n",
    "    noise_model += bs_noise_model / np.diag(bs_noise_model).mean() / len(stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "save_location = os.path.join(config.MODEL_DIR, args.subject)\n",
    "os.makedirs(save_location, exist_ok = True)\n",
    "np.savez(os.path.join(save_location, \"encoding_model_%s\" % args.gpt),\n",
    "    weights = weights, noise_model = noise_model, alphas = alphas, voxels = vox, stories = stories,\n",
    "    tr_stats = np.array(tr_stats), word_stats = np.array(word_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
