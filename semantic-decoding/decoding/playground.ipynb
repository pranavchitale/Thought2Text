{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "import config\n",
    "from GPT import GPT\n",
    "from StimulusModel import LMFeatures\n",
    "from utils_stim import get_stim\n",
    "from utils_resp import get_resp\n",
    "from utils_ridge.ridge import ridge, bootstrap_ridge\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get path errors, then change dir using os.chdir to 'whatever-your-root-is/semantic-decoding/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--subject\", type = str, required = True)\n",
    "parser.add_argument(\"--gpt\", type = str, default = \"perceived\")\n",
    "parser.add_argument(\"--sessions\", nargs = \"+\", type = int, default = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 18, 20])\n",
    "args = parser.parse_args(\"--subject S1\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training stories\n",
    "stories = []\n",
    "with open(os.path.join(config.DATA_TRAIN_DIR, \"sess_to_story.json\"), \"r\") as f:\n",
    "    sess_to_story = json.load(f) \n",
    "for sess in args.sessions:\n",
    "    stories.extend(sess_to_story[str(sess)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    " # load gpt\n",
    "with open(os.path.join(config.DATA_LM_DIR, args.gpt, \"vocab.json\"), \"r\") as f:\n",
    "    gpt_vocab = json.load(f)\n",
    "gpt = GPT(path = os.path.join(config.DATA_LM_DIR, args.gpt, \"model\"), vocab = gpt_vocab, device = config.GPT_DEVICE)\n",
    "features = LMFeatures(model = gpt, layer = config.GPT_LAYER, context_words = config.GPT_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate encoding model\n",
    "rstim, tr_stats, word_stats = get_stim(stories, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rresp = get_resp(args.subject, stories, stack = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27449, 81126)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rresp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nchunks = int(np.ceil(rresp.shape[0] / 5 / config.CHUNKLEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, alphas, bscorrs = bootstrap_ridge(rstim, rresp, use_corr = False, alphas = config.ALPHAS, nboots = config.NBOOTS, chunklen = config.CHUNKLEN, nchunks = nchunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3072, 81126)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/workspace/weights/S1.npy', weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "bscorrs = bscorrs.mean(2).max(0)\n",
    "vox = np.sort(np.argsort(bscorrs)[-config.VOXELS:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_dict = {story : get_stim([story], features, tr_stats = tr_stats) for story in stories}\n",
    "resp_dict = get_resp(args.subject, stories, stack = False, vox = vox)\n",
    "noise_model = np.zeros([len(vox), len(vox)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 138min 16.9s\n",
    "\n",
    "for hstory in stories:\n",
    "    tstim, hstim = np.vstack([stim_dict[tstory] for tstory in stories if tstory != hstory]), stim_dict[hstory]\n",
    "    tresp, hresp = np.vstack([resp_dict[tstory] for tstory in stories if tstory != hstory]), resp_dict[hstory]\n",
    "    bs_weights = ridge(tstim, tresp, alphas[vox])\n",
    "    resids = hresp - hstim.dot(bs_weights)\n",
    "    bs_noise_model = resids.T.dot(resids)\n",
    "    noise_model += bs_noise_model / np.diag(bs_noise_model).mean() / len(stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "save_location = os.path.join(config.MODEL_DIR, args.subject)\n",
    "os.makedirs(save_location, exist_ok = True)\n",
    "np.savez(os.path.join(save_location, \"encoding_model_%s\" % args.gpt), \n",
    "    weights = weights, noise_model = noise_model, alphas = alphas, voxels = vox, stories = stories,\n",
    "    tr_stats = np.array(tr_stats), word_stats = np.array(word_stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Wordrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "import config\n",
    "from utils_stim import get_story_wordseqs\n",
    "from utils_resp import get_resp\n",
    "from utils_ridge.DataSequence import DataSequence\n",
    "from utils_ridge.util import make_delayed\n",
    "from utils_ridge.ridge import bootstrap_ridge\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--subject\", type = str, required = True)\n",
    "parser.add_argument(\"--sessions\", nargs = \"+\", type = int, \n",
    "    default = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 18, 20])\n",
    "args = parser.parse_args(\"--subject S1\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training stories\n",
    "stories = []\n",
    "with open(os.path.join(config.DATA_TRAIN_DIR, \"sess_to_story.json\"), \"r\") as f:\n",
    "    sess_to_story = json.load(f) \n",
    "for sess in args.sessions:\n",
    "    stories.extend(sess_to_story[str(sess)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROI voxels\n",
    "with open(os.path.join(config.DATA_TRAIN_DIR, \"ROIs\", \"%s.json\" % args.subject), \"r\") as f:\n",
    "    vox = json.load(f)\n",
    "        \n",
    "# estimate word rate model\n",
    "save_location = os.path.join(config.MODEL_DIR, args.subject)\n",
    "os.makedirs(save_location, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordseqs = get_story_wordseqs(stories)\n",
    "rates = {}\n",
    "for story in stories:\n",
    "    ds = wordseqs[story]\n",
    "    words = DataSequence(np.ones(len(ds.data_times)), ds.split_inds, ds.data_times, ds.tr_times)\n",
    "    rates[story] = words.chunksums(\"lanczos\", window = 3)\n",
    "nz_rate = np.concatenate([rates[story][5+config.TRIM:-config.TRIM] for story in stories], axis = 0)\n",
    "nz_rate = np.nan_to_num(nz_rate).reshape([-1, 1])\n",
    "mean_rate = np.mean(nz_rate)\n",
    "rate = nz_rate - mean_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 35m 25.9s\n",
    "\n",
    "for roi in [\"speech\", \"auditory\"]:\n",
    "    resp = get_resp(args.subject, stories, stack = True, vox = vox[roi])\n",
    "    delresp = make_delayed(resp, config.RESP_DELAYS)\n",
    "    nchunks = int(np.ceil(delresp.shape[0] / 5 / config.CHUNKLEN))    \n",
    "    weights, _, _ = bootstrap_ridge(delresp, rate, use_corr = False,\n",
    "        alphas = config.ALPHAS, nboots = config.NBOOTS, chunklen = config.CHUNKLEN, nchunks = nchunks)\n",
    "    np.savez(os.path.join(save_location, \"word_rate_model_%s\" % roi), \n",
    "        weights = weights, mean_rate = mean_rate, voxels = vox[roi])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392020201/work/torch/csrc/tensor/python_tensor.cpp:451.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import argparse\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "\n",
    "import config\n",
    "from GPT import GPT\n",
    "from Decoder import Decoder, Hypothesis\n",
    "from LanguageModel import LanguageModel\n",
    "from EncodingModel import EncodingModel\n",
    "from StimulusModel import StimulusModel, get_lanczos_mat, affected_trs, LMFeatures\n",
    "from utils_stim import predict_word_rate, predict_word_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--subject\", type = str, required = True)\n",
    "parser.add_argument(\"--experiment\", type = str, required = True)\n",
    "parser.add_argument(\"--task\", type = str, required = True)\n",
    "args = parser.parse_args(\"--subject S1 --experiment perceived_speech --task wheretheressmoke\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine GPT checkpoint based on experiment\n",
    "if args.experiment in [\"imagined_speech\"]: gpt_checkpoint = \"imagined\"\n",
    "else: gpt_checkpoint = \"perceived\"\n",
    "\n",
    "# determine word rate model voxels based on experiment\n",
    "if args.experiment in [\"imagined_speech\", \"perceived_movies\"]: word_rate_voxels = \"speech\"\n",
    "else: word_rate_voxels = \"auditory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File(os.path.join(config.DATA_TEST_DIR, \"test_response\", args.subject, args.experiment, args.task + \".hf5\"), \"r\")\n",
    "resp = np.nan_to_num(hf[\"data\"][:])\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(291, 81126)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load gpt\n",
    "with open(os.path.join(config.DATA_LM_DIR, gpt_checkpoint, \"vocab.json\"), \"r\") as f:\n",
    "    gpt_vocab = json.load(f)\n",
    "with open(os.path.join(config.DATA_LM_DIR, \"decoder_vocab.json\"), \"r\") as f:\n",
    "    decoder_vocab = json.load(f)\n",
    "gpt = GPT(path = os.path.join(config.DATA_LM_DIR, gpt_checkpoint, \"model\"), vocab = gpt_vocab, device = config.GPT_DEVICE)\n",
    "features = LMFeatures(model = gpt, layer = config.GPT_LAYER, context_words = config.GPT_WORDS)\n",
    "lm = LanguageModel(gpt, decoder_vocab, nuc_mass = config.LM_MASS, nuc_ratio = config.LM_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models\n",
    "load_location = os.path.join(config.MODEL_DIR, args.subject)\n",
    "word_rate_model = np.load(os.path.join(load_location, \"word_rate_model_%s.npz\" % word_rate_voxels), allow_pickle = True)\n",
    "encoding_model = np.load(os.path.join(load_location, \"encoding_model_%s.npz\" % gpt_checkpoint))\n",
    "weights = encoding_model[\"weights\"]\n",
    "noise_model = encoding_model[\"noise_model\"]\n",
    "tr_stats = encoding_model[\"tr_stats\"]\n",
    "word_stats = encoding_model[\"word_stats\"]\n",
    "em = EncodingModel(resp, weights, encoding_model[\"voxels\"], noise_model, device = config.EM_DEVICE)\n",
    "em.set_shrinkage(config.NM_ALPHA)\n",
    "assert args.task not in encoding_model[\"stories\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/semantic-decoding/decoding/utils_ridge/interpdata.py:159: RuntimeWarning: invalid value encountered in divide\n",
      "  val = window * np.sin(np.pi*t) * np.sin(np.pi*t/window) / (np.pi**2 * t**2)\n"
     ]
    }
   ],
   "source": [
    "# predict word times\n",
    "word_rate = predict_word_rate(resp, word_rate_model[\"weights\"], word_rate_model[\"voxels\"], word_rate_model[\"mean_rate\"])\n",
    "if args.experiment == \"perceived_speech\": word_times, tr_times = predict_word_times(word_rate, resp, starttime = -10)\n",
    "else: word_times, tr_times = predict_word_times(word_rate, resp, starttime = 0)\n",
    "lanczos_mat = get_lanczos_mat(word_times, tr_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 169m 45.9s\n",
    "\n",
    "# decode responses\n",
    "decoder = Decoder(word_times, config.WIDTH)\n",
    "sm = StimulusModel(lanczos_mat, tr_stats, word_stats[0], device = config.SM_DEVICE)\n",
    "for sample_index in range(len(word_times)):\n",
    "    trs = affected_trs(decoder.first_difference(), sample_index, lanczos_mat)\n",
    "    ncontext = decoder.time_window(sample_index, config.LM_TIME, floor = 5)\n",
    "    beam_nucs = lm.beam_propose(decoder.beam, ncontext)\n",
    "    for c, (hyp, nextensions) in enumerate(decoder.get_hypotheses()):\n",
    "        nuc, logprobs = beam_nucs[c]\n",
    "        if len(nuc) < 1: continue\n",
    "        extend_words = [hyp.words + [x] for x in nuc]\n",
    "        extend_embs = list(features.extend(extend_words))\n",
    "        stim = sm.make_variants(sample_index, hyp.embs, extend_embs, trs)\n",
    "        likelihoods = em.prs(stim, trs)\n",
    "        local_extensions = [Hypothesis(parent = hyp, extension = x) for x in zip(nuc, logprobs, extend_embs)]\n",
    "        decoder.add_extensions(local_extensions, likelihoods, nextensions)\n",
    "    decoder.extend(verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.experiment in [\"perceived_movie\", \"perceived_multispeaker\"]: decoder.word_times += 10\n",
    "save_location = os.path.join(config.RESULT_DIR, args.subject, args.experiment)\n",
    "os.makedirs(save_location, exist_ok = True)\n",
    "decoder.save(os.path.join(save_location, args.task))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "import config\n",
    "from utils_eval import generate_null, load_transcript, windows, segment_data, WER, BLEU, METEOR, BERTSCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--subject\", type = str, required = True)\n",
    "parser.add_argument(\"--experiment\", type = str, required = True)\n",
    "parser.add_argument(\"--task\", type = str, required = True)\n",
    "parser.add_argument(\"--metrics\", nargs = \"+\", type = str, default = [\"WER\", \"BLEU\", \"METEOR\", \"BERT\"])\n",
    "parser.add_argument(\"--references\", nargs = \"+\", type = str, default = [])\n",
    "parser.add_argument(\"--null\", type = int, default = 10)\n",
    "args = parser.parse_args(\"--subject S1 --experiment perceived_speech --task wheretheressmoke\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(args.references) == 0:\n",
    "    args.references.append(args.task)\n",
    "    \n",
    "with open(os.path.join(config.DATA_TEST_DIR, \"eval_segments.json\"), \"r\") as f:\n",
    "    eval_segments = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/semantic-decoding/decoding/utils_eval.py:93: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  self.metric = load_metric(\"bleu\", keep_in_memory=True)\n",
      "/opt/conda/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for bleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/bleu/bleu.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc37d3b1ed54c34bd06bcbdc5360d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6600ebbc37f4e4394930bce01870989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for meteor contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/meteor/meteor.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dd003c4992b44a3a70d39b9db7804b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b1e5ea98c3d4f2c99ab0f39748d2baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31f94dc0bf6848b4b945a3ba07ccf4da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286ac5152dfa42a9a20ba46489f1cb34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4988b9b3254ad4947be6afa9bca83f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ef9c53befc247a490847b96ee22493e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309984d3606442a68a8f9999dadbbb9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 2m 52.1s\n",
    "\n",
    "# load language similarity metrics\n",
    "metrics = {}\n",
    "if \"WER\" in args.metrics: metrics[\"WER\"] = WER(use_score = True)\n",
    "if \"BLEU\" in args.metrics: metrics[\"BLEU\"] = BLEU(n = 1)\n",
    "if \"METEOR\" in args.metrics: metrics[\"METEOR\"] = METEOR()\n",
    "if \"BERT\" in args.metrics: metrics[\"BERT\"] = BERTSCORE(\n",
    "    idf_sents = np.load(os.path.join(config.DATA_TEST_DIR, \"idf_segments.npy\")), \n",
    "    rescale = False, \n",
    "    score = \"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load prediction transcript\n",
    "pred_path = os.path.join(config.RESULT_DIR, args.subject, args.experiment, args.task + \".npz\")\n",
    "pred_data = np.load(pred_path)\n",
    "pred_words, pred_times = pred_data[\"words\"], pred_data[\"times\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8m 8.9sec\n",
    "\n",
    "# generate null sequences\n",
    "if args.experiment in [\"imagined_speech\"]: gpt_checkpoint = \"imagined\"\n",
    "else: gpt_checkpoint = \"perceived\"\n",
    "null_word_list = generate_null(pred_times, gpt_checkpoint, args.null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "After applying the transforms on the reference and hypothesis sentences, their lengths must match. Instead got 49 reference and 56 hypothesis sentences.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 18\u001b[0m\n\u001b[1;32m     13\u001b[0m null_window_list \u001b[38;5;241m=\u001b[39m [segment_data(null_words, pred_times, window_cutoffs) \u001b[38;5;28;01mfor\u001b[39;00m null_words \u001b[38;5;129;01min\u001b[39;00m null_word_list]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mname, metric \u001b[38;5;129;01min\u001b[39;00m metrics\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# get null score for each window and the entire story\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     window_null_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([metric\u001b[38;5;241m.\u001b[39mscore(ref \u001b[38;5;241m=\u001b[39m ref_windows, pred \u001b[38;5;241m=\u001b[39m null_windows) \n\u001b[1;32m     19\u001b[0m                                     \u001b[38;5;28;01mfor\u001b[39;00m null_windows \u001b[38;5;129;01min\u001b[39;00m null_window_list])\n\u001b[1;32m     20\u001b[0m     story_null_scores \u001b[38;5;241m=\u001b[39m window_null_scores\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# get raw score and normalized score for each window\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[55], line 18\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m null_window_list \u001b[38;5;241m=\u001b[39m [segment_data(null_words, pred_times, window_cutoffs) \u001b[38;5;28;01mfor\u001b[39;00m null_words \u001b[38;5;129;01min\u001b[39;00m null_word_list]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mname, metric \u001b[38;5;129;01min\u001b[39;00m metrics\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# get null score for each window and the entire story\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     window_null_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mref_windows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnull_windows\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     19\u001b[0m                                     \u001b[38;5;28;01mfor\u001b[39;00m null_windows \u001b[38;5;129;01min\u001b[39;00m null_window_list])\n\u001b[1;32m     20\u001b[0m     story_null_scores \u001b[38;5;241m=\u001b[39m window_null_scores\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# get raw score and normalized score for each window\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/semantic-decoding/decoding/utils_eval.py:83\u001b[0m, in \u001b[0;36mWER.score\u001b[0;34m(self, ref, pred)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ref_seg, pred_seg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ref, pred):\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ref_seg) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m : error \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: error \u001b[38;5;241m=\u001b[39m \u001b[43mwer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref_seg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_seg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_score: scores\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m error)\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: use_score\u001b[38;5;241m.\u001b[39mappend(error)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/jiwer/measures.py:111\u001b[0m, in \u001b[0;36mwer\u001b[0;34m(reference, hypothesis, reference_transform, hypothesis_transform, truth, truth_transform)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03mCalculate the word error rate (WER) between one or more reference and\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03mhypothesis sentences.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m             hypothesis sentence(s).\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     97\u001b[0m (\n\u001b[1;32m     98\u001b[0m     reference,\n\u001b[1;32m     99\u001b[0m     hypothesis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m     hypothesis_transform\u001b[38;5;241m=\u001b[39mhypothesis_transform,\n\u001b[1;32m    109\u001b[0m )\n\u001b[0;32m--> 111\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_words\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypothesis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypothesis_transform\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\u001b[38;5;241m.\u001b[39mwer\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/jiwer/process.py:170\u001b[0m, in \u001b[0;36mprocess_words\u001b[0;34m(reference, hypothesis, reference_transform, hypothesis_transform)\u001b[0m\n\u001b[1;32m    165\u001b[0m hyp_transformed \u001b[38;5;241m=\u001b[39m _apply_transform(\n\u001b[1;32m    166\u001b[0m     hypothesis, hypothesis_transform, is_reference\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    167\u001b[0m )\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ref_transformed) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(hyp_transformed):\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter applying the transforms on the reference and hypothesis sentences, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtheir lengths must match. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstead got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ref_transformed)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m reference and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(hyp_transformed)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m hypothesis sentences.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    175\u001b[0m     )\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# Change each word into a unique character in order to compute\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# word-level levenshtein distance\u001b[39;00m\n\u001b[1;32m    179\u001b[0m ref_as_chars, hyp_as_chars \u001b[38;5;241m=\u001b[39m _word2char(ref_transformed, hyp_transformed)\n",
      "\u001b[0;31mValueError\u001b[0m: After applying the transforms on the reference and hypothesis sentences, their lengths must match. Instead got 49 reference and 56 hypothesis sentences."
     ]
    }
   ],
   "source": [
    "window_scores, window_zscores = {}, {}\n",
    "story_scores, story_zscores = {}, {}\n",
    "for reference in args.references:\n",
    "\n",
    "    # load reference transcript\n",
    "    ref_data = load_transcript(args.experiment, reference)\n",
    "    ref_words, ref_times = ref_data[\"words\"], ref_data[\"times\"]\n",
    "\n",
    "    # segment prediction and reference words into windows\n",
    "    window_cutoffs = windows(*eval_segments[args.task], config.WINDOW)\n",
    "    ref_windows = segment_data(ref_words, ref_times, window_cutoffs)\n",
    "    pred_windows = segment_data(pred_words, pred_times, window_cutoffs)\n",
    "    null_window_list = [segment_data(null_words, pred_times, window_cutoffs) for null_words in null_word_list]\n",
    "    \n",
    "    for mname, metric in metrics.items():\n",
    "\n",
    "        # get null score for each window and the entire story\n",
    "        window_null_scores = np.array([metric.score(ref = ref_windows, pred = null_windows) \n",
    "                                        for null_windows in null_window_list])\n",
    "        story_null_scores = window_null_scores.mean(1)\n",
    "\n",
    "        # get raw score and normalized score for each window\n",
    "        window_scores[(reference, mname)] = metric.score(ref = ref_windows, pred = pred_windows)\n",
    "        window_zscores[(reference, mname)] = (window_scores[(reference, mname)] \n",
    "                                                - window_null_scores.mean(0)) / window_null_scores.std(0)\n",
    "\n",
    "        # get raw score and normalized score for the entire story\n",
    "        story_scores[(reference, mname)] = metric.score(ref = ref_windows, pred = pred_windows)\n",
    "        story_zscores[(reference, mname)] = (story_scores[(reference, mname)].mean()\n",
    "                                                - story_null_scores.mean()) / story_null_scores.std()\n",
    "\n",
    "save_location = os.path.join(config.REPO_DIR, \"scores\", args.subject, args.experiment)\n",
    "os.makedirs(save_location, exist_ok = True)\n",
    "np.savez(os.path.join(save_location, args.task), \n",
    "            window_scores = window_scores, window_zscores = window_zscores, \n",
    "            story_scores = story_scores, story_zscores = story_zscores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
