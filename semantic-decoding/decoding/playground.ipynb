{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "import config\n",
    "from GPT import GPT\n",
    "from StimulusModel import LMFeatures\n",
    "from utils_stim import get_stim\n",
    "from utils_resp import get_resp\n",
    "from utils_ridge.ridge import ridge, bootstrap_ridge\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get path errors, then change dir using os.chdir to 'whatever-your-root-is/semantic-decoding/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--subject\", type = str, required = True)\n",
    "parser.add_argument(\"--gpt\", type = str, default = \"perceived\")\n",
    "parser.add_argument(\"--sessions\", nargs = \"+\", type = int, default = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 18, 20])\n",
    "args = parser.parse_args(\"--subject S1\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training stories\n",
    "stories = []\n",
    "with open(os.path.join(config.DATA_TRAIN_DIR, \"sess_to_story.json\"), \"r\") as f:\n",
    "    sess_to_story = json.load(f) \n",
    "for sess in args.sessions:\n",
    "    stories.extend(sess_to_story[str(sess)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# load gpt\n",
    "with open(os.path.join(config.DATA_LM_DIR, args.gpt, \"vocab.json\"), \"r\") as f:\n",
    "    gpt_vocab = json.load(f)\n",
    "gpt = GPT(path = os.path.join(config.DATA_LM_DIR, args.gpt, \"model\"), vocab = gpt_vocab, device = config.GPT_DEVICE)\n",
    "features = LMFeatures(model = gpt, layer = config.GPT_LAYER, context_words = config.GPT_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate encoding model\n",
    "rstim, tr_stats, word_stats = get_stim(stories, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rresp = get_resp(args.subject, stories, stack = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27449, 81126)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rresp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nchunks = int(np.ceil(rresp.shape[0] / 5 / config.CHUNKLEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, alphas, bscorrs = bootstrap_ridge(rstim, rresp, use_corr = False, alphas = config.ALPHAS, nboots = config.NBOOTS, chunklen = config.CHUNKLEN, nchunks = nchunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3072, 81126)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/workspace/weights/S1.npy', weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "bscorrs = bscorrs.mean(2).max(0)\n",
    "vox = np.sort(np.argsort(bscorrs)[-config.VOXELS:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_dict = {story : get_stim([story], features, tr_stats = tr_stats) for story in stories}\n",
    "resp_dict = get_resp(args.subject, stories, stack = False, vox = vox)\n",
    "noise_model = np.zeros([len(vox), len(vox)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 138min 16.9s\n",
    "\n",
    "for hstory in stories:\n",
    "    tstim, hstim = np.vstack([stim_dict[tstory] for tstory in stories if tstory != hstory]), stim_dict[hstory]\n",
    "    tresp, hresp = np.vstack([resp_dict[tstory] for tstory in stories if tstory != hstory]), resp_dict[hstory]\n",
    "    bs_weights = ridge(tstim, tresp, alphas[vox])\n",
    "    resids = hresp - hstim.dot(bs_weights)\n",
    "    bs_noise_model = resids.T.dot(resids)\n",
    "    noise_model += bs_noise_model / np.diag(bs_noise_model).mean() / len(stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "save_location = os.path.join(config.MODEL_DIR, args.subject)\n",
    "os.makedirs(save_location, exist_ok = True)\n",
    "np.savez(os.path.join(save_location, \"encoding_model_%s\" % args.gpt), \n",
    "    weights = weights, noise_model = noise_model, alphas = alphas, voxels = vox, stories = stories,\n",
    "    tr_stats = np.array(tr_stats), word_stats = np.array(word_stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Wordrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "import config\n",
    "from utils_stim import get_story_wordseqs\n",
    "from utils_resp import get_resp\n",
    "from utils_ridge.DataSequence import DataSequence\n",
    "from utils_ridge.util import make_delayed\n",
    "from utils_ridge.ridge import bootstrap_ridge\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--subject\", type = str, required = True)\n",
    "parser.add_argument(\"--sessions\", nargs = \"+\", type = int, \n",
    "    default = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 18, 20])\n",
    "args = parser.parse_args(\"--subject S1\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training stories\n",
    "stories = []\n",
    "with open(os.path.join(config.DATA_TRAIN_DIR, \"sess_to_story.json\"), \"r\") as f:\n",
    "    sess_to_story = json.load(f) \n",
    "for sess in args.sessions:\n",
    "    stories.extend(sess_to_story[str(sess)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROI voxels\n",
    "with open(os.path.join(config.DATA_TRAIN_DIR, \"ROIs\", \"%s.json\" % args.subject), \"r\") as f:\n",
    "    vox = json.load(f)\n",
    "        \n",
    "# estimate word rate model\n",
    "save_location = os.path.join(config.MODEL_DIR, args.subject)\n",
    "os.makedirs(save_location, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordseqs = get_story_wordseqs(stories)\n",
    "rates = {}\n",
    "for story in stories:\n",
    "    ds = wordseqs[story]\n",
    "    words = DataSequence(np.ones(len(ds.data_times)), ds.split_inds, ds.data_times, ds.tr_times)\n",
    "    rates[story] = words.chunksums(\"lanczos\", window = 3)\n",
    "nz_rate = np.concatenate([rates[story][5+config.TRIM:-config.TRIM] for story in stories], axis = 0)\n",
    "nz_rate = np.nan_to_num(nz_rate).reshape([-1, 1])\n",
    "mean_rate = np.mean(nz_rate)\n",
    "rate = nz_rate - mean_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 35m 25.9s\n",
    "\n",
    "for roi in [\"speech\", \"auditory\"]:\n",
    "    resp = get_resp(args.subject, stories, stack = True, vox = vox[roi])\n",
    "    delresp = make_delayed(resp, config.RESP_DELAYS)\n",
    "    nchunks = int(np.ceil(delresp.shape[0] / 5 / config.CHUNKLEN))    \n",
    "    weights, _, _ = bootstrap_ridge(delresp, rate, use_corr = False,\n",
    "        alphas = config.ALPHAS, nboots = config.NBOOTS, chunklen = config.CHUNKLEN, nchunks = nchunks)\n",
    "    np.savez(os.path.join(save_location, \"word_rate_model_%s\" % roi), \n",
    "        weights = weights, mean_rate = mean_rate, voxels = vox[roi])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import argparse\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "\n",
    "import config\n",
    "from GPT import GPT\n",
    "from Decoder import Decoder, Hypothesis\n",
    "from LanguageModel import LanguageModel\n",
    "from EncodingModel import EncodingModel\n",
    "from StimulusModel import StimulusModel, get_lanczos_mat, affected_trs, LMFeatures\n",
    "from utils_stim import predict_word_rate, predict_word_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--subject\", type = str, required = True)\n",
    "parser.add_argument(\"--experiment\", type = str, required = True)\n",
    "parser.add_argument(\"--task\", type = str, required = True)\n",
    "args = parser.parse_args(\"--subject S1 --experiment perceived_speech --task wheretheressmoke\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine GPT checkpoint based on experiment\n",
    "if args.experiment in [\"imagined_speech\"]: gpt_checkpoint = \"imagined\"\n",
    "else: gpt_checkpoint = \"perceived\"\n",
    "\n",
    "# determine word rate model voxels based on experiment\n",
    "if args.experiment in [\"imagined_speech\", \"perceived_movies\"]: word_rate_voxels = \"speech\"\n",
    "else: word_rate_voxels = \"auditory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File(os.path.join(config.DATA_TEST_DIR, \"test_response\", args.subject, args.experiment, args.task + \".hf5\"), \"r\")\n",
    "resp = np.nan_to_num(hf[\"data\"][:])\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(291, 81126)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load gpt\n",
    "with open(os.path.join(config.DATA_LM_DIR, gpt_checkpoint, \"vocab.json\"), \"r\") as f:\n",
    "    gpt_vocab = json.load(f)\n",
    "with open(os.path.join(config.DATA_LM_DIR, \"decoder_vocab.json\"), \"r\") as f:\n",
    "    decoder_vocab = json.load(f)\n",
    "gpt = GPT(path = os.path.join(config.DATA_LM_DIR, gpt_checkpoint, \"model\"), vocab = gpt_vocab, device = config.GPT_DEVICE)\n",
    "features = LMFeatures(model = gpt, layer = config.GPT_LAYER, context_words = config.GPT_WORDS)\n",
    "lm = LanguageModel(gpt, decoder_vocab, nuc_mass = config.LM_MASS, nuc_ratio = config.LM_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models\n",
    "load_location = os.path.join(config.MODEL_DIR, args.subject)\n",
    "word_rate_model = np.load(os.path.join(load_location, \"word_rate_model_%s.npz\" % word_rate_voxels), allow_pickle = True)\n",
    "encoding_model = np.load(os.path.join(load_location, \"encoding_model_%s.npz\" % gpt_checkpoint))\n",
    "weights = encoding_model[\"weights\"]\n",
    "noise_model = encoding_model[\"noise_model\"]\n",
    "tr_stats = encoding_model[\"tr_stats\"]\n",
    "word_stats = encoding_model[\"word_stats\"]\n",
    "em = EncodingModel(resp, weights, encoding_model[\"voxels\"], noise_model, device = config.EM_DEVICE)\n",
    "em.set_shrinkage(config.NM_ALPHA)\n",
    "assert args.task not in encoding_model[\"stories\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/chronos_data/rrao/Thought2Text/semantic-decoding/decoding/utils_ridge/interpdata.py:159: RuntimeWarning: invalid value encountered in divide\n",
      "  val = window * np.sin(np.pi*t) * np.sin(np.pi*t/window) / (np.pi**2 * t**2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1589"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict word times\n",
    "word_rate = predict_word_rate(resp, word_rate_model[\"weights\"], word_rate_model[\"voxels\"], word_rate_model[\"mean_rate\"])\n",
    "if args.experiment == \"perceived_speech\": word_times, tr_times = predict_word_times(word_rate, resp, starttime = -10)\n",
    "else: word_times, tr_times = predict_word_times(word_rate, resp, starttime = 0)\n",
    "lanczos_mat = get_lanczos_mat(word_times, tr_times)\n",
    "len(word_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stim.shape\n",
      "torch.Size([6, 6, 3072])\n",
      "trs.shape\n",
      "(6,)\n",
      "likelihoods.shape\n",
      "(6,)\n"
     ]
    }
   ],
   "source": [
    "# 169m 45.9s\n",
    "\n",
    "# decode responses\n",
    "decoder = Decoder(word_times, config.WIDTH)\n",
    "sm = StimulusModel(lanczos_mat, tr_stats, word_stats[0], device = config.SM_DEVICE)\n",
    "for sample_index in range(len(word_times)):\n",
    "    trs = affected_trs(decoder.first_difference(), sample_index, lanczos_mat)\n",
    "    ncontext = decoder.time_window(sample_index, config.LM_TIME, floor = 5)\n",
    "    beam_nucs = lm.beam_propose(decoder.beam, ncontext)\n",
    "    for c, (hyp, nextensions) in enumerate(decoder.get_hypotheses()):\n",
    "        nuc, logprobs = beam_nucs[c]\n",
    "        if len(nuc) < 1: continue\n",
    "        extend_words = [hyp.words + [x] for x in nuc]\n",
    "        extend_embs = list(features.extend(extend_words))\n",
    "        stim = sm.make_variants(sample_index, hyp.embs, extend_embs, trs)\n",
    "        print('stim.shape')\n",
    "        print(stim.shape)\n",
    "        print('trs.shape')\n",
    "        print(trs.shape)\n",
    "        likelihoods = em.prs(stim, trs)\n",
    "        print('likelihoods.shape')\n",
    "        print(likelihoods.shape)\n",
    "        local_extensions = [Hypothesis(parent = hyp, extension = x) for x in zip(nuc, logprobs, extend_embs)]\n",
    "        decoder.add_extensions(local_extensions, likelihoods, nextensions)\n",
    "        break\n",
    "    decoder.extend(verbose = False)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.experiment in [\"perceived_movie\", \"perceived_multispeaker\"]: decoder.word_times += 10\n",
    "save_location = os.path.join(config.RESULT_DIR, args.subject, args.experiment)\n",
    "os.makedirs(save_location, exist_ok = True)\n",
    "decoder.save(os.path.join(save_location, args.task))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "import config\n",
    "from utils_eval import generate_null, load_transcript, windows, segment_data, WER, BLEU, METEOR, BERTSCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--subject\", type = str, required = True)\n",
    "parser.add_argument(\"--experiment\", type = str, required = True)\n",
    "parser.add_argument(\"--task\", type = str, required = True)\n",
    "parser.add_argument(\"--metrics\", nargs = \"+\", type = str, default = [\"WER\", \"BLEU\", \"METEOR\", \"BERT\"])\n",
    "parser.add_argument(\"--references\", nargs = \"+\", type = str, default = [])\n",
    "parser.add_argument(\"--null\", type = int, default = 10)\n",
    "args = parser.parse_args(\"--subject S1 --experiment perceived_speech --task wheretheressmoke\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(args.references) == 0:\n",
    "    args.references.append(args.task)\n",
    "    \n",
    "with open(os.path.join(config.DATA_TEST_DIR, \"eval_segments.json\"), \"r\") as f:\n",
    "    eval_segments = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2m 52.1s\n",
    "\n",
    "# load language similarity metrics\n",
    "metrics = {}\n",
    "if \"WER\" in args.metrics: metrics[\"WER\"] = WER(use_score = True)\n",
    "if \"BLEU\" in args.metrics: metrics[\"BLEU\"] = BLEU(n = 1)\n",
    "if \"METEOR\" in args.metrics: metrics[\"METEOR\"] = METEOR()\n",
    "if \"BERT\" in args.metrics: metrics[\"BERT\"] = BERTSCORE(\n",
    "    idf_sents = np.load(os.path.join(config.DATA_TEST_DIR, \"idf_segments.npy\")), \n",
    "    rescale = False, \n",
    "    score = \"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load prediction transcript\n",
    "pred_path = os.path.join(config.RESULT_DIR, args.subject, args.experiment, args.task + \".npz\")\n",
    "pred_data = np.load(pred_path)\n",
    "pred_words, pred_times = pred_data[\"words\"], pred_data[\"times\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8m 8.9sec\n",
    "\n",
    "# generate null sequences\n",
    "if args.experiment in [\"imagined_speech\"]: gpt_checkpoint = \"imagined\"\n",
    "else: gpt_checkpoint = \"perceived\"\n",
    "null_word_list = generate_null(pred_times, gpt_checkpoint, args.null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "After applying the transforms on the reference and hypothesis sentences, their lengths must match. Instead got 49 reference and 56 hypothesis sentences.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 28\u001b[0m\n\u001b[1;32m     13\u001b[0m     null_window_list \u001b[38;5;241m=\u001b[39m [segment_data(null_words, pred_times, window_cutoffs) \u001b[38;5;28;01mfor\u001b[39;00m null_words \u001b[38;5;129;01min\u001b[39;00m null_word_list]\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m mname, metric \u001b[38;5;129;01min\u001b[39;00m metrics\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m         \u001b[38;5;66;03m# get null score for each window and the entire story\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m         \u001b[38;5;66;03m# get raw score and normalized score for the entire story\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m         story_scores[(reference, mname)] \u001b[38;5;241m=\u001b[39m \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mref_windows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpred_windows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;66;03m# story_zscores[(reference, mname)] = (story_scores[(reference, mname)].mean()\u001b[39;00m\n\u001b[1;32m     30\u001b[0m                                                 \u001b[38;5;66;03m# - story_null_scores.mean()) / story_null_scores.std()\u001b[39;00m\n\u001b[1;32m     32\u001b[0m save_location \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(config\u001b[38;5;241m.\u001b[39mREPO_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m\"\u001b[39m, args\u001b[38;5;241m.\u001b[39msubject, args\u001b[38;5;241m.\u001b[39mexperiment)\n",
      "File \u001b[0;32m/chronos_data/rrao/Thought2Text/semantic-decoding/decoding/utils_eval.py:83\u001b[0m, in \u001b[0;36mWER.score\u001b[0;34m(self, ref, pred)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ref_seg, pred_seg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ref, pred):\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ref_seg) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m : error \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: error \u001b[38;5;241m=\u001b[39m \u001b[43mwer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref_seg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_seg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_score: scores\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m error)\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: use_score\u001b[38;5;241m.\u001b[39mappend(error)\n",
      "File \u001b[0;32m/chronos_data/rrao/conda/envs/thought2text/lib/python3.10/site-packages/jiwer/measures.py:111\u001b[0m, in \u001b[0;36mwer\u001b[0;34m(reference, hypothesis, reference_transform, hypothesis_transform, truth, truth_transform)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03mCalculate the word error rate (WER) between one or more reference and\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03mhypothesis sentences.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m             hypothesis sentence(s).\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     97\u001b[0m (\n\u001b[1;32m     98\u001b[0m     reference,\n\u001b[1;32m     99\u001b[0m     hypothesis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m     hypothesis_transform\u001b[38;5;241m=\u001b[39mhypothesis_transform,\n\u001b[1;32m    109\u001b[0m )\n\u001b[0;32m--> 111\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_words\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypothesis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypothesis_transform\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\u001b[38;5;241m.\u001b[39mwer\n",
      "File \u001b[0;32m/chronos_data/rrao/conda/envs/thought2text/lib/python3.10/site-packages/jiwer/process.py:170\u001b[0m, in \u001b[0;36mprocess_words\u001b[0;34m(reference, hypothesis, reference_transform, hypothesis_transform)\u001b[0m\n\u001b[1;32m    165\u001b[0m hyp_transformed \u001b[38;5;241m=\u001b[39m _apply_transform(\n\u001b[1;32m    166\u001b[0m     hypothesis, hypothesis_transform, is_reference\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    167\u001b[0m )\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ref_transformed) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(hyp_transformed):\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter applying the transforms on the reference and hypothesis sentences, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtheir lengths must match. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstead got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ref_transformed)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m reference and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(hyp_transformed)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m hypothesis sentences.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    175\u001b[0m     )\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# Change each word into a unique character in order to compute\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# word-level levenshtein distance\u001b[39;00m\n\u001b[1;32m    179\u001b[0m ref_as_chars, hyp_as_chars \u001b[38;5;241m=\u001b[39m _word2char(ref_transformed, hyp_transformed)\n",
      "\u001b[0;31mValueError\u001b[0m: After applying the transforms on the reference and hypothesis sentences, their lengths must match. Instead got 49 reference and 56 hypothesis sentences."
     ]
    }
   ],
   "source": [
    "window_scores, window_zscores = {}, {}\n",
    "story_scores, story_zscores = {}, {}\n",
    "for reference in args.references:\n",
    "\n",
    "    # load reference transcript\n",
    "    ref_data = load_transcript(args.experiment, reference)\n",
    "    ref_words, ref_times = ref_data[\"words\"], ref_data[\"times\"]\n",
    "\n",
    "    # segment prediction and reference words into windows\n",
    "    window_cutoffs = windows(*eval_segments[args.task], config.WINDOW)\n",
    "    ref_windows = segment_data(ref_words, ref_times, window_cutoffs)\n",
    "    pred_windows = segment_data(pred_words, pred_times, window_cutoffs)\n",
    "    null_window_list = [segment_data(null_words, pred_times, window_cutoffs) for null_words in null_word_list]\n",
    "    \n",
    "    for mname, metric in metrics.items():\n",
    "\n",
    "        # get null score for each window and the entire story\n",
    "        window_null_scores = np.array([metric.score(ref = ref_windows, pred = null_windows) \n",
    "                                        for null_windows in null_window_list])\n",
    "        story_null_scores = window_null_scores.mean(1)\n",
    "\n",
    "        # get raw score and normalized score for each window\n",
    "        window_scores[(reference, mname)] = metric.score(ref = ref_windows, pred = pred_windows)\n",
    "        window_zscores[(reference, mname)] = (window_scores[(reference, mname)] \n",
    "                                                - window_null_scores.mean(0)) / window_null_scores.std(0)\n",
    "\n",
    "        # get raw score and normalized score for the entire story\n",
    "        story_scores[(reference, mname)] = metric.score(ref = ref_windows, pred = pred_windows)\n",
    "        story_zscores[(reference, mname)] = (story_scores[(reference, mname)].mean()\n",
    "                                                - story_null_scores.mean()) / story_null_scores.std()\n",
    "\n",
    "save_location = os.path.join(config.REPO_DIR, \"scores\", args.subject, args.experiment)\n",
    "os.makedirs(save_location, exist_ok = True)\n",
    "np.savez(os.path.join(save_location, args.task), \n",
    "            window_scores = window_scores, window_zscores = window_zscores, \n",
    "            story_scores = story_scores, story_zscores = story_zscores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
