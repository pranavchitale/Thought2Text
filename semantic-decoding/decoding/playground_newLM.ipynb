{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392020201/work/torch/csrc/tensor/python_tensor.cpp:451.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "import config\n",
    "from GPT import GPT\n",
    "from StimulusModel import LMFeatures\n",
    "from utils_stim import get_stim\n",
    "from utils_resp import get_resp\n",
    "from utils_ridge.ridge import ridge, bootstrap_ridge\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "import torch, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get path errors, then change dir using os.chdir to 'whatever-your-root-is/semantic-decoding/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--subject\", type = str, required = True)\n",
    "parser.add_argument(\"--gpt\", type = str, default = \"perceived\")\n",
    "parser.add_argument(\"--sessions\", nargs = \"+\", type = int, default = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 18, 20])\n",
    "args = parser.parse_args(\"--subject S1\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training stories\n",
    "stories = []\n",
    "with open(os.path.join(config.DATA_TRAIN_DIR, \"sess_to_story.json\"), \"r\") as f:\n",
    "    sess_to_story = json.load(f) \n",
    "for sess in args.sessions:\n",
    "    stories.extend(sess_to_story[str(sess)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    " # load gpt\n",
    "# with open(os.path.join(config.DATA_LM_DIR, args.gpt, \"vocab.json\"), \"r\") as f:\n",
    "#     gpt_vocab = json.load(f)\n",
    "\n",
    "# baseline\n",
    "# gpt = GPT(path = os.path.join(config.DATA_LM_DIR, args.gpt, \"model\"), vocab = gpt_vocab, device = config.GPT_DEVICE)\n",
    "# features = LMFeatures(model = gpt, layer = config.GPT_LAYER, context_words = config.GPT_WORDS)\n",
    "\n",
    "# ours\n",
    "gpt2_tokenizer = transformers.GPT2Tokenizer.from_pretrained(os.path.join(config.DATA_LM_DIR, 'gpt2_tokenizer'))\n",
    "gpt2_word_list = [None] * len(gpt2_tokenizer)\n",
    "for token, idx, in gpt2_tokenizer.get_vocab().items():\n",
    "    gpt2_word_list[idx] = token\n",
    "\n",
    "gpt = GPT(path = os.path.join(config.DATA_LM_DIR, \"gpt2_finetuned\"), vocab = gpt2_word_list, word2id = gpt2_tokenizer.get_vocab(), device = config.GPT_DEVICE)\n",
    "features = LMFeatures(model = gpt, layer = 4, context_words = config.GPT_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate encoding model\n",
    "rstim, tr_stats, word_stats = get_stim(stories, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rresp = get_resp(args.subject, stories, stack = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27449, 81126)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rresp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nchunks = int(np.ceil(rresp.shape[0] / 5 / config.CHUNKLEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27449, 3072), (27449, 81126))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rstim.shape, rresp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 138min 16.9s, 139m 45.6s\n",
    "# weights, alphas, bscorrs = bootstrap_ridge(rstim, rresp, use_corr = False, alphas = config.ALPHAS, nboots = 5, chunklen = config.CHUNKLEN, nchunks = nchunks)\n",
    "weights, alphas, bscorrs = bootstrap_ridge(rstim, rresp, use_corr = False, alphas = config.ALPHAS, nboots = config.NBOOTS, chunklen = config.CHUNKLEN, nchunks = nchunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3072, 81126)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bscorrs = bscorrs.mean(2).max(0)\n",
    "vox = np.sort(np.argsort(bscorrs)[-config.VOXELS:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_dict = {story : get_stim([story], features, tr_stats = tr_stats) for story in stories}\n",
    "resp_dict = get_resp(args.subject, stories, stack = False, vox = vox)\n",
    "noise_model = np.zeros([len(vox), len(vox)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 28m 52.7s\n",
    "\n",
    "for hstory in stories:\n",
    "    tstim, hstim = np.vstack([stim_dict[tstory] for tstory in stories if tstory != hstory]), stim_dict[hstory]\n",
    "    tresp, hresp = np.vstack([resp_dict[tstory] for tstory in stories if tstory != hstory]), resp_dict[hstory]\n",
    "    bs_weights = ridge(tstim, tresp, alphas[vox])\n",
    "    resids = hresp - hstim.dot(bs_weights)\n",
    "    bs_noise_model = resids.T.dot(resids)\n",
    "    noise_model += bs_noise_model / np.diag(bs_noise_model).mean() / len(stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save\n",
    "# save_location = os.path.join(config.MODEL_DIR, args.subject)\n",
    "# os.makedirs(save_location, exist_ok = True)\n",
    "# np.savez(os.path.join(save_location, \"encoding_model_%s\" % args.gpt), \n",
    "#     weights = weights, noise_model = noise_model, alphas = alphas, voxels = vox, stories = stories,\n",
    "#     tr_stats = np.array(tr_stats), word_stats = np.array(word_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "save_location = os.path.join(config.MODEL_DIR, args.subject)\n",
    "os.makedirs(save_location, exist_ok = True)\n",
    "np.savez(os.path.join(save_location, \"gpt2_encoding_model_%s\" % args.gpt), \n",
    "    weights = weights, noise_model = noise_model, alphas = alphas, voxels = vox, stories = stories,\n",
    "    tr_stats = np.array(tr_stats), word_stats = np.array(word_stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Wordrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "import config\n",
    "from utils_stim import get_story_wordseqs\n",
    "from utils_resp import get_resp\n",
    "from utils_ridge.DataSequence import DataSequence\n",
    "from utils_ridge.util import make_delayed\n",
    "from utils_ridge.ridge import bootstrap_ridge\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--subject\", type = str, required = True)\n",
    "parser.add_argument(\"--sessions\", nargs = \"+\", type = int, \n",
    "    default = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 18, 20])\n",
    "args = parser.parse_args(\"--subject S1\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training stories\n",
    "stories = []\n",
    "with open(os.path.join(config.DATA_TRAIN_DIR, \"sess_to_story.json\"), \"r\") as f:\n",
    "    sess_to_story = json.load(f) \n",
    "for sess in args.sessions:\n",
    "    stories.extend(sess_to_story[str(sess)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROI voxels\n",
    "with open(os.path.join(config.DATA_TRAIN_DIR, \"ROIs\", \"%s.json\" % args.subject), \"r\") as f:\n",
    "    vox = json.load(f)\n",
    "        \n",
    "# estimate word rate model\n",
    "save_location = os.path.join(config.MODEL_DIR, args.subject)\n",
    "os.makedirs(save_location, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordseqs = get_story_wordseqs(stories)\n",
    "rates = {}\n",
    "for story in stories:\n",
    "    ds = wordseqs[story]\n",
    "    words = DataSequence(np.ones(len(ds.data_times)), ds.split_inds, ds.data_times, ds.tr_times)\n",
    "    rates[story] = words.chunksums(\"lanczos\", window = 3)\n",
    "nz_rate = np.concatenate([rates[story][5+config.TRIM:-config.TRIM] for story in stories], axis = 0)\n",
    "nz_rate = np.nan_to_num(nz_rate).reshape([-1, 1])\n",
    "mean_rate = np.mean(nz_rate)\n",
    "rate = nz_rate - mean_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 35m 25.9s\n",
    "\n",
    "for roi in [\"speech\", \"auditory\"]:\n",
    "    resp = get_resp(args.subject, stories, stack = True, vox = vox[roi])\n",
    "    delresp = make_delayed(resp, config.RESP_DELAYS)\n",
    "    nchunks = int(np.ceil(delresp.shape[0] / 5 / config.CHUNKLEN))    \n",
    "    weights, _, _ = bootstrap_ridge(delresp, rate, use_corr = False,\n",
    "        alphas = config.ALPHAS, nboots = config.NBOOTS, chunklen = config.CHUNKLEN, nchunks = nchunks)\n",
    "    np.savez(os.path.join(save_location, \"word_rate_model_%s\" % roi), \n",
    "        weights = weights, mean_rate = mean_rate, voxels = vox[roi])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import argparse\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "\n",
    "import config\n",
    "from GPT import GPT\n",
    "from Decoder import Decoder, Hypothesis\n",
    "from LanguageModel import LanguageModel\n",
    "from EncodingModel import EncodingModel\n",
    "from StimulusModel import StimulusModel, get_lanczos_mat, affected_trs, LMFeatures\n",
    "from utils_stim import predict_word_rate, predict_word_times\n",
    "\n",
    "import transformers, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--subject\", type = str, required = True)\n",
    "parser.add_argument(\"--experiment\", type = str, required = True)\n",
    "parser.add_argument(\"--task\", type = str, required = True)\n",
    "args = parser.parse_args(\"--subject S1 --experiment perceived_speech --task wheretheressmoke\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine GPT checkpoint based on experiment\n",
    "if args.experiment in [\"imagined_speech\"]: gpt_checkpoint = \"imagined\"\n",
    "else: gpt_checkpoint = \"perceived\"\n",
    "\n",
    "# determine word rate model voxels based on experiment\n",
    "if args.experiment in [\"imagined_speech\", \"perceived_movies\"]: word_rate_voxels = \"speech\"\n",
    "else: word_rate_voxels = \"auditory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File(os.path.join(config.DATA_TEST_DIR, \"test_response\", args.subject, args.experiment, args.task + \".hf5\"), \"r\")\n",
    "resp = np.nan_to_num(hf[\"data\"][:])\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(291, 81126)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# load gpt\n",
    "# ours\n",
    "gpt2_tokenizer = transformers.GPT2Tokenizer.from_pretrained(os.path.join(config.DATA_LM_DIR, 'gpt2_tokenizer'))\n",
    "gpt2_word_list = [None] * len(gpt2_tokenizer)\n",
    "for token, idx, in gpt2_tokenizer.get_vocab().items():\n",
    "    gpt2_word_list[idx] = token\n",
    "\n",
    "gpt = GPT(path = os.path.join(config.DATA_LM_DIR, \"gpt2_pretrained_noeos\"), vocab = gpt2_word_list, word2id = gpt2_tokenizer.get_vocab(), device = config.GPT_DEVICE)\n",
    "features = LMFeatures(model = gpt, layer = 4, context_words = config.GPT_WORDS)\n",
    "\n",
    "# with open(os.path.join(config.DATA_LM_DIR, gpt_checkpoint, \"vocab.json\"), \"r\") as f:\n",
    "#     gpt_vocab = json.load(f)\n",
    "with open(os.path.join(config.DATA_LM_DIR, \"decoder_vocab.json\"), \"r\") as f:\n",
    "    decoder_vocab = json.load(f)\n",
    "# gpt = GPT(path = os.path.join(config.DATA_LM_DIR, gpt_checkpoint, \"model\"), vocab = gpt_vocab, device = config.GPT_DEVICE)\n",
    "# features = LMFeatures(model = gpt, layer = config.GPT_LAYER, context_words = config.GPT_WORDS)\n",
    "lm = LanguageModel(gpt, decoder_vocab, nuc_mass = config.LM_MASS, nuc_ratio = config.LM_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models\n",
    "load_location = os.path.join(config.MODEL_DIR, args.subject)\n",
    "word_rate_model = np.load(os.path.join(load_location, \"word_rate_model_%s.npz\" % word_rate_voxels), allow_pickle = True)\n",
    "# encoding_model = np.load(os.path.join(load_location, \"encoding_model_%s.npz\" % gpt_checkpoint))\n",
    "encoding_model = np.load(os.path.join(load_location, \"gpt2_encoding_model_%s.npz\" % gpt_checkpoint))\n",
    "weights = encoding_model[\"weights\"]\n",
    "noise_model = encoding_model[\"noise_model\"]\n",
    "tr_stats = encoding_model[\"tr_stats\"]\n",
    "word_stats = encoding_model[\"word_stats\"]\n",
    "em = EncodingModel(resp, weights, encoding_model[\"voxels\"], noise_model, device = config.EM_DEVICE)\n",
    "em.set_shrinkage(config.NM_ALPHA)\n",
    "assert args.task not in encoding_model[\"stories\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/Thought2Text/semantic-decoding/decoding/utils_ridge/interpdata.py:159: RuntimeWarning: invalid value encountered in divide\n",
      "  val = window * np.sin(np.pi*t) * np.sin(np.pi*t/window) / (np.pi**2 * t**2)\n"
     ]
    }
   ],
   "source": [
    "# predict word times\n",
    "word_rate = predict_word_rate(resp, word_rate_model[\"weights\"], word_rate_model[\"voxels\"], word_rate_model[\"mean_rate\"])\n",
    "if args.experiment == \"perceived_speech\": word_times, tr_times = predict_word_times(word_rate, resp, starttime = -10)\n",
    "else: word_times, tr_times = predict_word_times(word_rate, resp, starttime = 0)\n",
    "lanczos_mat = get_lanczos_mat(word_times, tr_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LanguageModel(gpt, decoder_vocab, nuc_mass = config.LM_MASS, nuc_ratio = config.LM_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1589 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1589/1589 [1:47:07<00:00,  4.05s/it]\n"
     ]
    }
   ],
   "source": [
    "# 169m 45.9s\n",
    "# word_rate is the number of words spoken in an observation. List sums to total number of words in the transcript.\n",
    "# word_times is the timestamp in seconds at which a word was spoken. Size = sum(word_rate).\n",
    "\n",
    "# ctr = 0\n",
    "# start = time.time()\n",
    "# decode responses\n",
    "decoder = Decoder(word_times, 15)#config.WIDTH)\n",
    "sm = StimulusModel(lanczos_mat, tr_stats, word_stats[0], device = config.SM_DEVICE)\n",
    "for sample_index in tqdm.tqdm(range(len(word_times))):\n",
    "    trs = affected_trs(decoder.first_difference(), sample_index, lanczos_mat)\n",
    "    ncontext = decoder.time_window(sample_index, config.LM_TIME, floor = 5)\n",
    "    beam_nucs = lm.beam_propose(decoder.beam, ncontext)\n",
    "    # if ctr == 15:\n",
    "    #     break\n",
    "    # ctr += 1\n",
    "    for c, (hyp, nextensions) in enumerate(decoder.get_hypotheses()):\n",
    "        nuc, logprobs = beam_nucs[c]\n",
    "        if len(nuc) < 1: continue\n",
    "        extend_words = [hyp.words + [x] for x in nuc]\n",
    "        extend_embs = list(features.extend(extend_words))\n",
    "        stim = sm.make_variants(sample_index, hyp.embs, extend_embs, trs)\n",
    "        likelihoods = em.prs(stim, trs)\n",
    "        local_extensions = [Hypothesis(parent = hyp, extension = x) for x in zip(nuc, logprobs, extend_embs)]\n",
    "        decoder.add_extensions(local_extensions, likelihoods, nextensions)\n",
    "\n",
    "    decoder.extend(verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.experiment in [\"perceived_movie\", \"perceived_multispeaker\"]: decoder.word_times += 10\n",
    "save_location = os.path.join(config.RESULT_DIR, args.subject, args.experiment)\n",
    "os.makedirs(save_location, exist_ok = True)\n",
    "decoder.save(os.path.join(save_location, args.task+'_gpt2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "import config\n",
    "from utils_eval import generate_null, load_transcript, windows, segment_data, WER, BLEU, METEOR, BERTSCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--subject\", type = str, required = True)\n",
    "parser.add_argument(\"--experiment\", type = str, required = True)\n",
    "parser.add_argument(\"--task\", type = str, required = True)\n",
    "parser.add_argument(\"--metrics\", nargs = \"+\", type = str, default = [\"WER\", \"BLEU\", \"METEOR\", \"BERT\"])\n",
    "parser.add_argument(\"--references\", nargs = \"+\", type = str, default = [])\n",
    "parser.add_argument(\"--null\", type = int, default = 10)\n",
    "args = parser.parse_args(\"--subject S1 --experiment perceived_speech --task wheretheressmoke\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(args.references) == 0:\n",
    "    args.references.append(args.task)\n",
    "    \n",
    "with open(os.path.join(config.DATA_TEST_DIR, \"eval_segments.json\"), \"r\") as f:\n",
    "    eval_segments = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for bleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/bleu/bleu.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for meteor contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/meteor/meteor.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 2m 52.1s or 8.6s\n",
    "\n",
    "# load language similarity metrics\n",
    "metrics = {}\n",
    "if \"WER\" in args.metrics: metrics[\"WER\"] = WER(use_score = True)\n",
    "if \"BLEU\" in args.metrics: metrics[\"BLEU\"] = BLEU(n = 1)\n",
    "if \"METEOR\" in args.metrics: metrics[\"METEOR\"] = METEOR()\n",
    "if \"BERT\" in args.metrics: metrics[\"BERT\"] = BERTSCORE(\n",
    "    idf_sents = np.load(os.path.join(config.DATA_TEST_DIR, \"idf_segments.npy\")), \n",
    "    rescale = False, \n",
    "    score = \"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load prediction transcript\n",
    "pred_path = os.path.join(config.RESULT_DIR, args.subject, args.experiment, args.task+'_gpt2' + \".npz\")\n",
    "pred_data = np.load(pred_path)\n",
    "pred_words, pred_times = pred_data[\"words\"], pred_data[\"times\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8m 8.9sec\n",
    "\n",
    "# generate null sequences\n",
    "if args.experiment in [\"imagined_speech\"]: gpt_checkpoint = \"imagined\"\n",
    "else: gpt_checkpoint = \"perceived\"\n",
    "null_word_list = generate_null(pred_times, gpt_checkpoint, args.null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# null_word_list = np.load('null_word_list.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_scores, window_zscores = {}, {}\n",
    "story_scores, story_zscores = {}, {}\n",
    "for reference in args.references:\n",
    "\n",
    "    # load reference transcript\n",
    "    ref_data = load_transcript(args.experiment, reference)\n",
    "    ref_words, ref_times = ref_data[\"words\"], ref_data[\"times\"]\n",
    "\n",
    "    # segment prediction and reference words into windows\n",
    "    window_cutoffs = windows(*eval_segments[args.task], config.WINDOW)\n",
    "    ref_windows = segment_data(ref_words, ref_times, window_cutoffs)\n",
    "    pred_windows = segment_data(pred_words, pred_times, window_cutoffs)\n",
    "    null_window_list = [segment_data(null_words, pred_times, window_cutoffs) for null_words in null_word_list]\n",
    "    \n",
    "    for mname, metric in metrics.items():\n",
    "\n",
    "        # get null score for each window and the entire story\n",
    "        window_null_scores = np.array([metric.score(ref = ref_windows, pred = null_windows) \n",
    "                                        for null_windows in null_window_list])\n",
    "        story_null_scores = window_null_scores.mean(1)\n",
    "\n",
    "        # get raw score and normalized score for each window\n",
    "        window_scores[(reference, mname)] = metric.score(ref = ref_windows, pred = pred_windows)\n",
    "        window_zscores[(reference, mname)] = (window_scores[(reference, mname)] \n",
    "                                                - window_null_scores.mean(0)) / window_null_scores.std(0)\n",
    "\n",
    "        # get raw score and normalized score for the entire story\n",
    "        story_scores[(reference, mname)] = metric.score(ref = ref_windows, pred = pred_windows)\n",
    "        story_zscores[(reference, mname)] = (story_scores[(reference, mname)].mean()\n",
    "                                                - story_null_scores.mean()) / story_null_scores.std()\n",
    "\n",
    "save_location = os.path.join(config.REPO_DIR, \"scores\", args.subject, args.experiment)\n",
    "os.makedirs(save_location, exist_ok = True)\n",
    "np.savez(os.path.join(save_location, args.task+'_gpt2'), \n",
    "            window_scores = window_scores, window_zscores = window_zscores, \n",
    "            story_scores = story_scores, story_zscores = story_zscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('wheretheressmoke', 'WER'): -13.071730618874115,\n",
       " ('wheretheressmoke', 'BLEU'): -22.25487294995505,\n",
       " ('wheretheressmoke', 'METEOR'): -16.06553233150744,\n",
       " ('wheretheressmoke', 'BERT'): -33.929707}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story_zscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('wheretheressmoke', 'WER'): 1.9219846660381588,\n",
       " ('wheretheressmoke', 'BLEU'): -0.5513700296316832,\n",
       " ('wheretheressmoke', 'METEOR'): 1.2374410749343323,\n",
       " ('wheretheressmoke', 'BERT'): 2.7528594}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story_zscores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
