import os, sys, random, re, collections, string
from datasets import load_dataset

import numpy as np
import re

import torch
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from torch.nn import BCELoss

from tqdm.auto import tqdm

import sklearn.model_selection
import sklearn.metrics
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score
import heapq

import matplotlib.pyplot as plt

import transformers
from transformers import GPT2LMHeadModel, GPT2TokenizerFast, get_scheduler

# https://huggingface.co/datasets/cnn_dailymail - Can/need to cite this
dataset = load_dataset('cnn_dailymail', '3.0.0')

modelname = 'distilbert/distilgpt2'
gpt2_tokenizer = GPT2TokenizerFast.from_pretrained('distilgpt2')
gpt2_model = GPT2LMHeadModel.from_pretrained('distilbert/distilgpt2')


# tokenizer = AutoTokenizer.from_pretrained(modelname)
# model = AutoModelForSeq2SeqLM.from_pretrained(modelname)
model = gpt2_model
tokenizer = gpt2_tokenizer
tokenizer.pad_token = tokenizer.eos_token

# data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=modelname)    
# rouge = evaluate.load("rouge")


device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

class cnnDataSet(Dataset):
    def __init__(self, data):
        self.data = data
        self.id = self.data['id']
        self.article = self.data['article']
        self.highlights = self.data['highlights']
        self.input_ids = torch.tensor(self.data['input_ids'])
        self.attention_mask = torch.tensor(self.data['attention_mask'])

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx],  self.attention_mask[idx], torch.sum(self.attention_mask[idx])



def split_article(art):
    splitstr = re.split(' -- ', art, 1)
    # print(splitstr)
    if len(splitstr) == 1 or (re.search(r'(\([\w+]+\))|([A-Z][A-Z]+)|(\([\w\s]+\))|(\([\w]+\.[\w]+\))', splitstr[0])) is None:
        return art
    else:
        return splitstr[1]

    
def tokenize_str(examples):
    # inputs = [doc for doc in examples["article"]]
    inputs = []
    # labs = examples['highlights']
    
    inputs = [split_article(art) for art in examples['article']]

    # model_inputs = gpt2_tokenizer(inputs, max_length=1024, truncation=True)
    # labels = gpt2_tokenizer(text_target=labs, max_length=128, truncation=True)

    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, return_tensors='pt', padding='max_length')
    # labels = tokenizer(text_target=labs, max_length=128, truncation=True)

    # model_inputs["labels"] = labels["input_ids"]
    return model_inputs


def train_model(train_dataset, plot_loss_curves=True, model=gpt2_model):
    '''
    Method to fine tune and train the passed model
    Returns nothing
    train_dataset - The datset whose text is in the format PASSAGE\nQUESTION?ANSWER
    '''

    # train_dataset = GPT2CustomDataset(training_text, training_labels, gpt2_tokenizer)
    # unique_labels = train_dataset.get_unique_labels()
    # unique_labels_encoding = {lab:gpt2_tokenizer.encode(lab)[0] for lab in unique_labels}
    # Hyperparameters
    batch_sz=3
    learn_rate = 1e-5
    wt_decay = 1e-5
    num_epochs = 2

    train_dataloader = DataLoader(train_dataset, batch_size=batch_sz)

    # torch.set_default_device(device)
    
    # model.to(dtype=torch.float16)
    model.to(device)
    optimizer = AdamW(model.parameters(), lr=learn_rate, weight_decay=wt_decay)
    # lossfunc = binary_cross_entropy_with_logits().to(device)
    # print(model.parameters())
    dataloader_len = len(train_dataloader)
    num_training_steps = num_epochs * dataloader_len
    
    lr_scheduler = get_scheduler(
        name="linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps
    )
    
    progress_bar = tqdm(range(num_training_steps))
    loss_list = []

    # Use FP16 for speed purposes
    scaler = torch.cuda.amp.GradScaler()
    # Update loss and other values every 3 iterations
    accum_iter = 3
    
    torch.cuda.empty_cache()
    model.train()
    for epoch in range(num_epochs):
        
        epoch_loss = []
        for  batch_idx, batch in enumerate(train_dataloader):
            with torch.set_grad_enabled(True):


                max_valid_input, max_valid_attention = trunc_batch(batch)
                if ((batch_idx + 1) % accum_iter == 0) or (batch_idx + 1 == dataloader_len):                    
                    optimizer.zero_grad()

                with torch.cuda.amp.autocast():
                    
                    output = model(
                                   input_ids = max_valid_input.to(device),
                                   attention_mask=max_valid_attention.to(device),
                                   labels=max_valid_input.to(device)
                                   )
                    loss = output.loss/accum_iter
                
                # loss.backward()
                scaler.scale(loss).backward()
                # model.float()
                if ((batch_idx + 1) % accum_iter == 0) or (batch_idx + 1 == dataloader_len):
                    # optimizer.step()
                    scaler.step(optimizer)
                    # scaler.step(lr_scheduler)
                    # optimizer.zero_grad()
                    scaler.update()
                    lr_scheduler.step()
            
            progress_bar.update(1)
            if plot_loss_curves:
                loss_list.append(loss.item())
                
    if plot_loss_curves:
        plot_loss_curve(loss_list, f'distil-gpt2 LM LR : {learn_rate}')


def trunc_batch(batch):
    max_pad = torch.max(batch[2])
    # What we do here is that for all tensor in the batch, we truncate the attention and padding to the length of the 
    # maximum valid length, so save time and memory on computing the softmaxes
    input_ids = torch.stack([b[:max_pad] for b in batch[0]])
    attention_masks = torch.stack([b[:max_pad] for b in batch[1]])

    return input_ids, attention_masks

def plot_loss_curve(losslist, rword, save_title = 'loss_curve_a2_p2.png'):
    l = len(losslist)
    plt.plot(range(l), losslist)
    plt.title(f'Plotting the loss across Iterations for "{rword}" ')    
    plt.xlabel('Iterations')
    plt.ylabel('Loss')
    # plt.show()
    plt.savefig(save_title)


def main():
    # Just training the first 10k articles for now
    train = dataset['train'].select(range(10000))
    test = dataset['test'].select(range(1000))
    validation = dataset['validation']
    tokenized_train = train.map(tokenize_str, batched=True)
    tokenized_test = test.map(tokenize_str, batched=True)

    tokenized_train_dataset = cnnDataSet(tokenized_train)
    train_model(tokenized_train_dataset, plot_loss_curves=True)
    
    # https://pytorch.org/tutorials/beginner/saving_loading_models.html
    torch.save(model, 'fineTunedDistilGPT2_cnnDaily')
    

    